# 白话机器学习中的数学

机器学习中最麻烦的地方，就是收集数据。无论收集数据的环境变得多好，还是有很多需要人工介入的工作。

机器学习擅长的领域：

● 回归（regression）
● 分类（classification）
● 聚类（clustering）

## 回归

回归就是在处理**连续数据**如时间序列数据时使用的技术。（如股价预测）

## 分类

鉴别垃圾邮件就是一种分类问题。

只有两个类别的问题称为二分类，有三个及以上的问题称为多分类，比如数字的识别就属于多分类问题 ( 识别数字 0 - 9 )。

## 聚类

 它与分类的区别在于数据带不带标签。也有人把标签称为正确答案数据。（如鉴别垃圾邮件中需要标记是否为垃圾邮件）

## 有监督学习和无监督学习

有监督学习：使用有标签的数据进行的学习。（回归和分类）

无监督学习：使用没有标签的数据进行的学习称为无监督学习。（聚类）



# 基于广告费预测点击量（回归）

从数据中进行学习，然后给出预测值。

## 定义模型

先从简单的开始，广告费越多，点击量越高。（一次函数y=ax+b，x是广告费，y是点击量），改写成以下格式：

$$
f_\theta(x)=\theta_0 + \theta_1x
$$
将训练数据中的广告费代入fθ(x)，把得到的点击量与训练数据中的点击量相比较，然后找出使二者的差最小的θ。

$f_\theta(x)$和$x$已知，利用训练数据找出$\theta$的最优值，最理想情况$y-f_\theta(x)=0$。(找到与训练数据最拟合的一条直线，预测结果)

误差：$E(\theta)=\frac{1}{2}\sum_{i=1}^n(y^{(i)}-f_\theta(x^{(i)}))^2$  , $y^{(i)}$和$x^{(i)}$中的$i$不是$i$次幂的意思，而是指第$i$个训练数据。

计算平方是考虑到误差为负值的情况，方便计算和微分，同样$\frac{1}{2}$也是为了方便微分随便加的常数（乘以正的常数，函数的形状就会被横向压扁或者纵向拉长，但函数本身取最小值的点是不变的）。



## 最小二乘法

修改参数$\theta$，使误差变得越来越小，这种做法称为最小二乘法。

### 最速下降法

举例：表达式为$g(x)=(x-1)^2$的二次函数，它的最小值是g(x)=0，出现在x=1时。

先微分：$g^{'}_x=2(x-1)$，$x>1$时，$g(x)$递增，$x<1$时，$g(x)$递减。

在$x=3$时，为了使$g(x)$的值变小，需要向左移动x，也就是减小x。同样，$x=-1$时，需要右移动x。

只要向与导数的符号相反的方向移动x，g(x)就会自然而然地沿着最小值的方向前进了。也即**最速下降法或者梯度下降法**。

$x:=x-\eta g^{'}_x$ ，$A:=B$ 意思是通过B来定义A。（该公式表示将x与导数的符号相反的方向移动）

$\eta$称为学习率的正的常数。根据学习率的大小，到达最小值的更新次数也会发生变化。换种说法就是收敛速度会不同。有时候甚至会出现完全无法收敛，一直发散的情况。( $\eta$ 的值表示移动x的步伐多大，或者说跳跃程度)

如果η较大，那么$x:=x-\eta (2x-2)$会在两个值上跳来跳去，甚至有可能远离最小值。这就是发散状态。而当η较小时，移动量也变小，更新次数就会增加，但是值确实是会朝着收敛的方向而去。

$E(\theta)=\frac{1}{2}\sum_{i=1}^n(y^{(i)}-f_\theta(x^{(i)}))^2$ 与上式类似，但有两个变量$\theta_0$和$\theta_1$，所以需要使用偏微分。

$\theta_0:=\theta_0-\eta\frac{\partial E}{\partial\theta_0}$

$\theta_1:=\theta_1-\eta\frac{\partial E}{\partial\theta_1}$

计算，令$u=E(\theta), v=f(\theta)$，求得
$$
\theta_0:=\theta_0-\eta\sum_{i=1}^{n}(f_\theta(x^{(i)})-y^{(i)}) \\ \theta_1:=\theta_1-\eta\sum_{i=1}^{n}(f_\theta(x^{(i)})-y^{(i)})x^{(i)}
$$




## 多项式回归

将直线改为曲线，曲线比直线拟合得更好。

$f_\theta(x)=\theta_0+\theta_1x+\theta_2x^2$

或者更大次数$f_\theta(x)=\theta_0+\theta_1x+\theta_2x^2+\theta_3x^3+...+\theta_nx^n$，虽然次数越大拟合得越好，但难免也会出现过拟合的问题。

像这样**增加函数中多项式的次数**，然后再使用函数的分析方法被称为多项式回归。

求$\theta$：

$\theta_0:=\theta_0-\eta\sum_{i=1}^{n}(f_\theta(x^{(i)})-y^{(i)})$

$\theta_1:=\theta_1-\eta\sum_{i=1}^{n}(f_\theta(x^{(i)})-y^{(i)})x^{(i)}$

$\theta_2:=\theta_2-\eta\sum_{i=1}^{n}(f_\theta(x^{(i)})-y^{(i)})x^{(i)^2}$



## 多重回归

之前只是根据广告费来预测点击量，现在呢，决定点击量的除了广告费之外，还有广告的展示位置和广告版面的大小等多个要素。

为了让问题尽可能地简单，这次我们只考虑广告版面的大小，设广告费为x1、广告栏的宽为x2、广告栏的高为x3，那么fθ可以表示如下:
$f_\theta(x1,x2,x3)=\theta_0+\theta_1x_1+\theta_2x_2+\theta_3x_3$

同样，分别求目标函数对$\theta_0$, ···, $\theta_3$的偏微分，然后更新参数就行了。

$\theta_j:=\theta_j-\eta\sum_{i=1}^{n}(f_\theta(x^{(i)})-y^{(i)})x^{(i)}_j$

像这样包含了多个变量的回归称为多重回归。



## 随机梯度下降法

所谓的最速下降法就是对所有的训练数据都重复进行计算，计算量大、计算时间长、容易陷入局部最优解（找到的是极值而不是最值）是最速下降法的缺点。

在随机梯度下降法中会随机选择一个训练数据，并使用它来更新参数。

$\theta_j:=\theta_j-\eta(f_\theta(x^{(k)})-y^{(k)})x^{(k)}_j$

随机梯度下降法由于训练数据是随机选择的，更新参数时使用的又是选择数据时的梯度，所以不容易陷入目标函数的局部最优解。

我们前面提到了随机选择1个训练数据的做法，此外还有随机选择m个训练数据来更新参数的做法，被称为小批量（mini-batch）梯度下降法。



# 学习分类基于图像大小进行分类

需求：根据图像的宽高判断图片是纵向还是横向。

算法：通过训练找到权重向量，然后得到与这个向量垂直的直线，最后根据这条直线就可以对数据进行分类了。

## 感知机

如何求出权重向量呢？

基本做法和回归时相同：将权重向量用作参数，创建更新表达式来更新参数。接下来要说明的就是被称为感知机（perceptron）的模型。

感知机是接受多个输入后将每个值与各自的权重相乘，最后输出总和的模型。

感知机是非常简单的模型，基本不会应用在实际的问题中，但它是神经网络和深度学习的基础模型。**简单感知机（单层感知机）缺点是只能解决线性可分（使用直线分类）的问题。**

**多层感知机就是神经网络了。**

### 训练数据的准备

设表示宽的轴为$x_1$、表示高的轴为$x_2$，用$y$来表示图像是横向还是纵向的，横向的值为1、纵向的值为-1。

根据参数向量$x$来判断图像是横向还是纵向的函数，即返回1或者-1的函数$f_w(x)$的定义如下。这个函数被称为判别函数。

$f_w(x)=\begin{cases} 1 , (w \cdot x\geq0) \\ -1,(w \cdot x<0) \end{cases}$

**w**为权重向量。$w \cdot x=|w|\cdot|x|\cdot cos\theta$，在90◦＜θ＜270◦的时候cos θ为负，其他为正，正好分割为两部分作为分类。

权重向量的更新表达式：

$w:=\begin{cases} w+y^{(i)}x^{(i)} , (f_w(x^{(i)}) \neq y^{(i)}) \\ w,((f_w(x^{(i)}) = y^{(i)}) \end{cases}$



## 逻辑回归

把分类作为概率来考虑，这里设横向的值为1、纵向的值为0。在学习感知机时之所以设置值为1和-1，是因为这样会使参数更新表达式看起来更简洁，而现在则是设置为1和0会更简洁。

这里的思路是一样的。我们需要能够将未知数据分类为某个类别的函数fθ(x)。

sigmoid函数：$f_\theta(x)=\frac{1}{1+exp(-\theta^Tx)}$，sigmoid函数的取值范围是0＜fθ(x)＜1，所以它可以作为概率来使用。



### 决策边界

刚才说到把表达式$f_\theta(x)$当作概率来使用，那么接下来我们就把未知数据x是横向图像的概率作为$f_\theta(x)$。其表达式是这样的：

$P(y=1|x)=f_\theta(x)$，概率论中的条件概率，即在给出x数据时y=1( 图像为横向 )的概率。

以0.5为阈值，然后把$f_\theta(x)$的结果与它相比较，从而分类横向或纵向。

$y=\begin{cases} 1 , (f_\theta(x)\geq0.5)\\0 , (f_\theta(x) < 0.5) \end{cases}$  ，当 $\theta^Tx\geq0$时，$f_\theta \geq0.5$

我们将$\theta^Tx=0$这条直线作为边界线，就可以把这条线两侧的数据分类为横向和纵向了，这样用于数据分类的直线称为决策边界。

为了求得正确的参数$\theta$而定义目标函数，进行微分，然后求参数的更新表达式，这种算法就称为逻辑回归。



### 似然函数

y=1的时候，我们希望概率P(y=1|x)是最大的，y=1的时候，我们希望概率P(y=1|x)是最大的，其中P(y=1|x)是图像为横向的概率，P(y=0|x)是图像为纵向的概率。

假定所有的训练数据都是互不影响、独立发生的，这种情况下整体的概率就可以用下面的联合概率来表示。

$L(\theta)=P(y^{(1)}=0|x^{(1)})P(y^{(2)}=0|x^{(2)})P(y^{(3)}=0|x^{(3)})...P(y^{(6)}=1|x^{(6)})$

想一想扔2次骰子的情况。第1次的结果是1点，且第2次的结果是2点的概率是多少呢？首先1点出现的概率是$1/6$，接下来2点出现的概率是$1/6$，二者连续发生的概率就要使用乘法计算$1/36$。

联合概率的表达式是可以一般化的，写法如下：
$L(\theta)=\prod_{i=1}^{n}P(y^{(i)}=1|x^{(i)})^{y^{(i)}}P(y^{(i)}=0|x^{(i)})^{1-y^{(i)}}$

这里的目标函数$L(θ)$也被称为似然，函数的名字L取自似然的英文单词Likelihood的首字母，它的意思是最近似的。

我们可以认为似然函数L(θ)中，使其值最大的参数θ能够最近似地说明训练数据。



### 对数似然函数

因为log是单调递增函数，所以我们现在考察的似然函数也是在L(θ1)＜L(θ2)时，有log L(θ1)＜logL(θ2)成立。也就是说，使L(θ)最大化等价于使log L(θ)最大化。
$$
logL(\theta)= log\prod_{i=1}^{n}P(y^{(i)}=1|x^{(i)})^{y^{(i)}}P(y^{(i)}=0|x^{(i)})^{1-y^{(i)}} \\
=\sum_{i=1}^{n}(y^{(i)}logP(y^{(i)}=1|x^{(i)})+(1-y^{(i)})log(1-P(y^{(i)}=1|x^{(i)}))) \\
=\sum_{i=1}^{n}(y^{(i)}logf_\theta(x^{(i)})+(1-y^{(i)})log(1-f_\theta(x^{(i)})))
$$

### 似然函数的微分

参数更新表达式：

$\theta j:=\theta j+\eta \sum_{i=1}^{n}(y^{(i)}-f_\theta(x^{(i)}))x_j^{(i)}$

现在是以最大化为目标，所以必须按照与最小化时相反的方向移动参数。



# 建立评估模型

...



# 使用Python实现

## 回归

数据准备`click.csv`：

```csv
x,y
235,591
216,539
148,413
35,310
85,308
204,519
49,325
25,332
173,498
191,498
134,392
99,334
117,385
112,387
162,425
272,659
159,400
159,427
59,319
198,522
```

安装依赖包：

```bash
pip3 install numpy matplotlib
```

绘图：

```python
import numpy as np
import  matplotlib.pyplot as plt

train = np.loadtxt("click.csv", delimiter=",", skiprows=1)

train_x = train[:,0]
train_y = train[:,1]

plt.plot(train_x, train_y, 'o')
plt.show()
```

### 作为一次函数实现

$$
f_\theta(x)=\theta_0+\theta_1x \\
E(\theta)=\frac{1}{2}\sum_{i=1}^{n}(y^{(i)}-f_\theta(x^{(i)}))^2
$$

$$
\theta_0:=\theta_0-\eta\sum_{i=1}^{n}(f_\theta(x^{(i)})-y^{(i)}) \\ \theta_1:=\theta_1-\eta\sum_{i=1}^{n}(f_\theta(x^{(i)})-y^{(i)})x^{(i)}
$$

```python
# coding=UTF-8

import numpy as np
import  matplotlib.pyplot as plt

train = np.loadtxt("click.csv", delimiter=",", skiprows=1)

train_x = train[:,0]
train_y = train[:,1]

# plt.plot(train_x, train_y, 'o')
# plt.show()

# 参数初始化，生成一个 0~1 的小数
theta0 = np.random.rand()
theta1 = np.random.rand()

# 预测函数
def f(x):
    return theta0 + theta1*x

# 目标函数
def E(x, y):
    return 0.5*np.sum((y-f(x))**2)

"""
把训练数据变成平均值为0、方差为1的数据。
这个预处理不是必须的，但是做了之后，参数的收敛会更快。
这种做法也被称为标准化或者z-score规范化。
变换表达式：z^(i) = (x^(i) - µ) / σ
µ是训练数据的平均值，σ是标准差
"""
mu = train_x.mean()
sigma = train_x.std()
def standardize(x):
    return (x-mu) / sigma

# 只有横轴的刻度改变了，方便参数收敛
train_z = standardize(train_x)
# plt.plot(train_z, train_y, 'o')
# plt.show()

# 学习率
ETA = 1e-3

# 误差的差值
diff = 1

# 更新次数
count = 0 

# 重复学习
error = E(train_z, train_y)
while diff > 1e-2:
    # 更新结果保存到临时变量
    tmp0 = theta0 - ETA*np.sum((f(train_z) - train_y))
    tmp1 = theta1 - ETA*np.sum((f(train_z) - train_y) * train_z)
    # 更新参数
    theta0 = tmp0
    theta1 = tmp1
    # 计算与上一次误差的差值
    current_error = E(train_z, train_y)
    diff = abs(error - current_error)
    error = current_error
    # 输出日志
    count += 1
    log = '第{}次: theta0 = {:.3f}, theta1 = {:.3f}, 差值 = {:.4f}'
    print(log.format(count, theta0, theta1, diff))

x = np.linspace(-3, 3, 100)
plt.plot(train_z, train_y, 'o')
plt.plot(x, f(x))
plt.show()

```

### 多项式回归的实现

$f_\theta(x)=\theta_0+\theta_1x+\theta_2x^2$

在之前的基础上，增加参数$\theta_2$，并替换预测函数。
$$
\theta = \left[
\matrix{
\theta_0 \\
\theta_1 \\
\theta_2 \\
}
\right]

x^{(i)} = \left[
\matrix{
1 \\
x^{(i)} \\
x^{(i)^2} \\
}
\right]
$$
由于训练数据有很多，所以我们把1行数据当作1个训练数据，以矩阵的形式来处理会更好。
$$
X = \left[\matrix{x^{(1)^{T}} \\ x^{(2)^{T}} \\ x^{(3)^{T}} \\ ... \\ x^{(n)^{T}}}\right] = \left[\matrix{1 \quad x^{(1)} \quad x^{(1)^2} \\ 1 \quad x^{(2)} \quad x^{(2)^2} \\ 1 \quad x^{(3)} \quad x^{(3)^2} \\ ... \\ 1 \quad x^{(n)} \quad x^{(n)^2} }\right]
$$
然后，再求这个矩阵与参数向量θ的积。
$$
X\theta = \left[\matrix{1 \quad x^{(1)} \quad x^{(1)^2} \\ 1 \quad x^{(2)} \quad x^{(2)^2} \\ 1 \quad x^{(3)} \quad x^{(3)^2} \\ ... \\ 1 \quad x^{(n)} \quad x^{(n)^2} }\right] 

\left[
\matrix{
\theta_0 \\
\theta_1 \\
\theta_2 \\
}
\right]
=
\left[
\matrix{
\theta_0 + \theta_1x^{(1)} + \theta_2x^{(1)^2} \\
\theta_0 + \theta_1x^{(2)} + \theta_2x^{(2)^2} \\
\theta_0 + \theta_1x^{(3)} + \theta_2x^{(3)^2} \\
... \\
\theta_0 + \theta_1x^{(n)} + \theta_2x^{(n)^2}
}
\right]
$$

```python
# coding=UTF-8

import numpy as np
import  matplotlib.pyplot as plt

train = np.loadtxt("click.csv", delimiter=",", skiprows=1)

train_x = train[:,0]
train_y = train[:,1]

# plt.plot(train_x, train_y, 'o')
# plt.show()

# 参数初始化，生成一个 0~1 的小数
theta0 = np.random.rand()
theta1 = np.random.rand()
theta2 = np.random.rand()

# 预测函数
def f(x):
    return theta0 + theta1*x + theta2*x*x

# 目标函数
def E(x, y):
    return 0.5*np.sum((y-f(x))**2)

"""
把训练数据变成平均值为0、方差为1的数据。
这个预处理不是必须的，但是做了之后，参数的收敛会更快。
这种做法也被称为标准化或者z-score规范化。
变换表达式：z^(i) = (x^(i) - µ) / σ
µ是训练数据的平均值，σ是标准差
"""
mu = train_x.mean()
sigma = train_x.std()
def standardize(x):
    return (x-mu) / sigma

# 只有横轴的刻度改变了，方便参数收敛
train_z = standardize(train_x)
# plt.plot(train_z, train_y, 'o')
# plt.show()

# 学习率
ETA = 1e-3

# 误差的差值
diff = 1

# 更新次数
count = 0 

# 重复学习
error = E(train_z, train_y)
while diff > 1e-2:
    # 更新结果保存到临时变量
    tmp0 = theta0 - ETA*np.sum((f(train_z) - train_y))
    tmp1 = theta1 - ETA*np.sum((f(train_z) - train_y) * train_z)
    tmp2 = theta2 - ETA*np.sum((f(train_z) - train_y) * train_z * train_z)
    # 更新参数
    theta0 = tmp0
    theta1 = tmp1
    theta2 = tmp2

    # 计算与上一次误差的差值
    current_error = E(train_z, train_y)
    diff = abs(error - current_error)
    error = current_error
    # 输出日志
    count += 1
    log = '第{}次: theta0 = {:.3f}, theta1 = {:.3f}, theta2 = {:.3f} 差值 = {:.4f}'
    print(log.format(count, theta0, theta1, theta2, diff))

x = np.linspace(-3, 3, 100)
plt.plot(train_z, train_y, 'o')
plt.plot(x, f(x))
plt.show()
```

### 随机梯度下降法的实现

...



## 分类 - 感知机

...

## 分类 - 逻辑回归

...

## 正则化

...

